import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.preprocessing import MinMaxScaler, StandardScaler  # para datos sesgados o uniformes
from scipy import stats   #para transformaciones
from sklearn.model_selection import train_test_split   #para entrenar modelo
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score   #metricas para evaluar modelo

df = pd.read_csv('lm393.csv')

df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True)

#  Convertir a la hora de Chile (Chile/Santiago)
df['timestamp'] = df['timestamp'].dt.tz_convert('America/Santiago')

#  Graficar datos de intensidad de luz para identificar la distribuci贸n de los datos
"""
plt.figure(figsize=(10, 6))
sns.histplot(df, kde=True, bins=30)
plt.xlabel("intensidad de luz")
plt.ylabel("Frecuencia")
plt.title("Datos de Sensor de Intensidad de Luz")
plt.legend()
plt.grid(True)
plt.xticks(rotation=45)
plt.show()
"""

#De acuerdo al grafico anterior los datos tienden a un sesgo positivo
#escala los valores entre 0 y 1
#Aplicar Transformaci贸n para Reducir el Sesgo

#Dado que la distribuci贸n es asim茅trica, podemos probar diferentes transformaciones:

#Opci贸n 1: Transformaci贸n Logar铆tmica (煤til para distribuciones con valores muy grandes)
#Ventaja: Comprime los valores grandes y expande los peque帽os, reduciendo el sesgo.
df['log_luz'] = np.log1p(df['analog_value'])  # log(1 + x) para evitar log(0)

#Opci贸n 2: Transformaci贸n Box-Cox (mejor si hay valores cercanos a 0)
#Ventaja: Encuentra autom谩ticamente la mejor transformaci贸n para normalizar los datos.
df['boxcox_luz'], lambda_bc = stats.boxcox(df['analog_value'] + 1)  # +1 para evitar valores 0

#Normalizar los Datos (si es necesario)
#Si queremos que los valores est茅n en un rango espec铆fico (0-1) para modelos como redes neuronales:
#til para modelos de Machine Learning sensibles a la escala de los datos.
scaler = MinMaxScaler()  # O StandardScaler()  
df["analog_val"] = scaler.fit_transform(df[["analog_value"]])


#Volver a Graficar los Datos Transformados
#Para verificar si la transformaci贸n ayud贸, podemos graficar los nuevos datos:
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# Histograma original
sns.histplot(df['analog_value'], bins=50, kde=True, ax=axes[0])
axes[0].set_title("Distribuci贸n Original")

# Histograma despu茅s de transformaci贸n
sns.histplot(df['log_luz'], bins=50, kde=True, ax=axes[1])
axes[1].set_title("Distribuci贸n Transformada (Log)")

#plt.show()

#Si la nueva distribuci贸n es m谩s sim茅trica y con forma de campana, la transformaci贸n fue efectiva.


#Preparar datos para entrenar el modelo
#Para entrenar un modelo, necesitamos dividir los datos en variables de entrada (X) y salida (y)
#Nota: Si timestamp es num茅rico, puede ser 煤til extraer caracter铆sticas como hora del d铆a, d铆a de la semana, etc. en lugar de usarlo directamente.
# Seleccionar caracter铆sticas (X) y variable objetivo (y)
df['timestamp'] = df['timestamp'].view('int64') / 1e9  # Convertir a segundos desde la 茅poca
X = df[['timestamp']]  # colocar si hay m谩s variables disponibles
y = df['log_luz']   #transformacion seleccionada  

# Dividir en datos de entrenamiento y prueba (80% entrenamiento, 20% prueba)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#Seleccionar y Entrenar un Modelo
#Para predicci贸n de intensidad de luz (regresi贸n), podemos usar modelos como Regresi贸n Lineal o Random Forest.
#Opci贸n 1: Regresi贸n Lineal
modelo = LinearRegression()

# Entrenar el modelo con los datos de entrenamiento
modelo.fit(X_train, y_train)

# Hacer predicciones
#y_pred = modelo.predict(X_test)

#Opci贸n 2: Random Forest (mejor para datos no lineales)
modelo = RandomForestRegressor(n_estimators=100, random_state=42)

# Entrenar
modelo.fit(X_train, y_train)

# Predecir
y_pred = modelo.predict(X_test)



#Evaluar el Modelo
#Para evaluar qu茅 tan bien predice el modelo, usamos m茅tricas como:
#Valores m谩s bajos de MAE y MSE indican mejor precisi贸n.
# Un R虏 cercano a 1 significa que el modelo es bueno para predecir.
print("MAE:", mean_absolute_error(y_test, y_pred))  # Error absoluto medio
print("MSE:", mean_squared_error(y_test, y_pred))  # Error cuadr谩tico medio
print("R虏 Score:", r2_score(y_test, y_pred))  # Qu茅 tan bien ajusta el modelo


#Visualizar Resultados
#Para ver c贸mo se comparan las predicciones con los valores reales:
plt.figure(figsize=(10, 5))
plt.plot(y_test.values, label="Valor Real", linestyle="dashed")
plt.plot(y_pred, label="Predicci贸n", alpha=0.7)
plt.legend()
plt.xlabel("Tiempo")
plt.ylabel("Intensidad de Luz")
plt.title("Comparaci贸n entre Valores Reales y Predichos")
plt.show()



#  Calcular y mostrar la media de la intensidad de luz
#print(f"Estadisticas de intensidad de luz: {df['analog_value'].describe()}")
#print(df.info())